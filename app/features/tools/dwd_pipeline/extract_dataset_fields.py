"""
extract_dataset_fields.py — Field Inventory Generator for ClimaStation

This script analyzes the canonical station profile generated by build_station_summary.py
and extracts a comprehensive list of unique field names (both raw and metadata) per dataset.

Inputs:
- data/5_matching/station_profile_canonical.pretty.json → matches raw files to metadata

Output:
- data/6_fields/dataset_fields.json → canonical field list grouped by dataset

Debug:
- data/0_debug/extract_dataset_fields_debug.log

Main features:
- Aggregates all canonical raw field names across raw files
- Extracts all metadata keys from matched metadata rows
- Builds a clean, sorted inventory of dataset-specific fields

This inventory supports:
- Schema creation and refinement
- Canonical field mapping verification
- Detection of field anomalies across datasets

Author: ClimaStation Team
"""

import json
import logging
from pathlib import Path
from collections import defaultdict

# === Paths ===
INPUT_PATH = Path("data/5_matching/station_profile_canonical.pretty.json")
OUTPUT_PATH = Path("data/6_fields/dataset_fields.json")
DEBUG_LOG_PATH = Path("data/0_debug/extract_dataset_fields_debug.log")

# === Logging setup ===
OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
if DEBUG_LOG_PATH.exists():
    DEBUG_LOG_PATH.unlink()

logging.basicConfig(
    filename=DEBUG_LOG_PATH,
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logging.info("=== Starting extract_dataset_fields.py ===")

def extract_dataset_fields(profile_path: Path, output_path: Path):
    with open(profile_path, "r", encoding="utf-8") as f:
        profile_data = json.load(f)

    dataset_fields = defaultdict(set)

    for station_id, datasets in profile_data.items():
        for dataset_name, dataset in datasets.items():
            for raw_file, file_info in dataset.get("raw_files", {}).items():
                raw_fields = file_info.get("raw_fields_canonical", [])
                dataset_fields[dataset_name].update(raw_fields)
                logging.debug(f"{dataset_name} ← raw fields: {raw_fields}")

                for meta_rows in file_info.get("matched_metadata", {}).values():
                    for row in meta_rows:
                        metadata_fields = [k for k in row.keys() if isinstance(k, str)]
                        dataset_fields[dataset_name].update(metadata_fields)
                        logging.debug(f"{dataset_name} ← metadata fields: {metadata_fields}")

    cleaned_dataset_fields = {
        dataset: sorted(f for f in fields if isinstance(f, str))
        for dataset, fields in dataset_fields.items()
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(cleaned_dataset_fields, f, indent=2, ensure_ascii=False)

    logging.info(f"✔ Written dataset field inventory to: {output_path}")
    print(f"✅ Field inventory written to: {output_path}")

if __name__ == "__main__":
    extract_dataset_fields(INPUT_PATH, OUTPUT_PATH)
